{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_raw = []\n",
    "dataset_files = []\n",
    "for file in os.listdir(\"/root/robocup/submission_data\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        datasets_raw.append(pd.read_csv(f\"/root/robocup/submission_data/{file}\"))\n",
    "        dataset_files.append(file)\n",
    "\n",
    "for index, dataset in enumerate(datasets_raw):\n",
    "    datasets_raw[index] = dataset.iloc[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = {\n",
    "        \"AEteam\": 0,\n",
    "        \"CYRUS\": 1,\n",
    "        \"FRA-UNIted\": 2,\n",
    "        \"HELIOS2024\": 3,\n",
    "        \"ITAndroids\": 4,\n",
    "        \"Mars\": 5,\n",
    "        \"Oxsy\": 6,\n",
    "        \"R2D2\": 7,\n",
    "        \"RoboCIn\": 8,\n",
    "        \"YuShan2024\": 9,\n",
    "    }\n",
    "    df[\"l_name\"] = df[\"l_name\"].map(d)\n",
    "    df[\"r_name\"] = df[\"r_name\"].map(d)\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    agents = [f\"l{id}\" for id in range(1, 12)] + [f\"r{id}\" for id in range(1, 12)]\n",
    "    b = [\"vx\", \"vy\", \"t\", \"body\", \"neck\", \"vwidth\", \"stamina\"]\n",
    "    drop_cols = [f\"{a}_{b}\" for a, b in itertools.product(agents, b)]\n",
    "    drop_cols += [\"b_vx\", \"b_vy\"]\n",
    "    drop_cols += [\n",
    "        \"#\",\n",
    "        \"cycle\",\n",
    "        \"stopped\",\n",
    "        \"playmode\",\n",
    "        \"l_score\",\n",
    "        \"l_pen_score\",\n",
    "        \"r_score\",\n",
    "        \"r_pen_score\",\n",
    "    ]\n",
    "    return df.drop(columns=drop_cols)\n",
    "\n",
    "\n",
    "def frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"frame\"] = df.reset_index().index\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_data(df: pd.DataFrame) -> list[pd.DataFrame]:\n",
    "    df = one_hot(df)\n",
    "    df = frame(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "for index, dataset in enumerate(datasets_raw):\n",
    "    datasets_raw[index] = process_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMax:\n",
    "    def __init__(self, min, max):\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "        if self.min >= self.max:\n",
    "            raise ValueError(\"min must be less than max\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return (x - self.min) / (self.max - self.min)\n",
    "\n",
    "    def inverse(self, x):\n",
    "        return x * (self.max - self.min) + self.min\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MinMax({self.min}, {self.max})\"\n",
    "\n",
    "\n",
    "def swap_rl(df):\n",
    "    df[\"l_name\"], df[\"r_name\"] = df[\"r_name\"], df[\"l_name\"]\n",
    "    df[\"b_x\"] *= -1\n",
    "\n",
    "    for i in range(1, 12):\n",
    "        l_x, r_x = f\"l{i}_x\", f\"r{i}_x\"\n",
    "        l_y, r_y = f\"l{i}_y\", f\"r{i}_y\"\n",
    "\n",
    "        df[l_x], df[r_x] = -df[r_x].values, -df[l_x].values\n",
    "        df[l_y], df[r_y] = df[r_y].values, df[l_y].values\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "min_max_d = np.load(\"/root/robocup/datas/min_max_d.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_merge_datasets(datas):\n",
    "    data_list = []\n",
    "    for data in datas:\n",
    "        df = pd.DataFrame(data)\n",
    "        data_list.append(df)\n",
    "    datas = pd.concat(data_list)\n",
    "    return datas\n",
    "\n",
    "\n",
    "def name_onehot(dfs):\n",
    "    for i in range(10):\n",
    "        dfs[f\"l_name_{i}\"] = dfs[\"l_name\"] == i\n",
    "        dfs[f\"r_name_{i}\"] = dfs[\"r_name\"] == i\n",
    "        dfs[f\"l_name_{i}\"] = (dfs[\"l_name\"] == i).astype(int)\n",
    "        dfs[f\"r_name_{i}\"] = (dfs[\"r_name\"] == i).astype(int)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def drop_unnecessary_columns(dfs):\n",
    "    dfs = dfs.drop(\n",
    "        columns=[\n",
    "            \"#\",\n",
    "            \"cycle\",\n",
    "            \"stopped\",\n",
    "            \"playmode\",\n",
    "            \"l_name\",\n",
    "            \"r_name\",\n",
    "            # \"goal_type\",\n",
    "            \"l_score\",\n",
    "            \"r_score\",\n",
    "            \"l_pen_score\",\n",
    "            \"r_pen_score\",\n",
    "        ]\n",
    "    )\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def min_max_normalize(dfs, min_max_d=None):\n",
    "    if min_max_d is None:\n",
    "        min_max_d = {col: MinMax(min(dfs[col]), max(dfs[col])) for col in dfs.columns}\n",
    "    for col in dfs.columns:\n",
    "        dfs[col] = min_max_d[col](dfs[col])\n",
    "    return dfs, min_max_d\n",
    "\n",
    "\n",
    "def revert_min_max_normalize(dfs, min_max_d):\n",
    "    for col in dfs.columns:\n",
    "        dfs[col] = min_max_d[col].inverse(dfs[col])\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def divide_dataframe(dfs, df_size=50):\n",
    "    df_list = []\n",
    "\n",
    "    for i in range(0, len(dfs), df_size):\n",
    "        df = dfs.iloc[i : i + df_size]\n",
    "        df_list.append(df)\n",
    "    return df_list\n",
    "\n",
    "\n",
    "def list_to_numpy(dfs: list) -> np.ndarray:\n",
    "    cols = dfs[0].columns\n",
    "    return np.array([df.values for df in dfs]).astype(np.float32), cols\n",
    "\n",
    "\n",
    "def revert_numpy_from_list(dfs: np.ndarray, cols) -> list[pd.DataFrame]:\n",
    "    return [pd.DataFrame(df, columns=cols) for df in dfs]\n",
    "\n",
    "\n",
    "submission = datasets_raw\n",
    "submission = clean_and_merge_datasets(submission)\n",
    "# display(submission.iloc[:100])\n",
    "submission = name_onehot(submission)\n",
    "submission = drop_unnecessary_columns(submission)\n",
    "submission, _ = min_max_normalize(submission, min_max_d)\n",
    "submission = divide_dataframe(submission, df_size=20)\n",
    "\n",
    "submission, cols = list_to_numpy(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "import visualizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "PROJECT_NAME = \"RobocupTrajectoryPrediction_DataExpanded\"\n",
    "MODEL_NAME = \"BiGRU_dataExpanded_2\"\n",
    "GROUP_NAME = \"BiGRU\"\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYER = 4\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuclideanDistance(torchmetrics.Metric):\n",
    "    def __init__(self, cols, min_max_d, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cols = cols\n",
    "        self.min_max_d = min_max_d\n",
    "        self.add_state(\"sum\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"count\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self._mins = torch.tensor(\n",
    "            [min_max_d[col].min for col in cols], dtype=torch.float32\n",
    "        )\n",
    "        self._maxs = torch.tensor(\n",
    "            [min_max_d[col].max for col in cols], dtype=torch.float32\n",
    "        )\n",
    "        self._scale = self._maxs - self._mins\n",
    "\n",
    "    def update(self, preds, target):\n",
    "        indices = torch.cat([torch.tensor([0]), torch.arange(5, 201, 9)])\n",
    "        final_preds = preds[:, -1, :]\n",
    "        final_target = target[:, -1, :]\n",
    "\n",
    "        inversed_preds = torch.zeros_like(final_preds)\n",
    "        inversed_target = torch.zeros_like(final_target)\n",
    "\n",
    "        mins = self._mins.to(device=device, dtype=final_preds.dtype)\n",
    "        scale = self._scale.to(device=device, dtype=final_preds.dtype)\n",
    "\n",
    "        inversed_preds = final_preds.clone()\n",
    "        inversed_target = final_target.clone()\n",
    "        n_cols = len(self.cols)\n",
    "        inversed_preds[:, :n_cols] = final_preds[:, :n_cols] * scale + mins\n",
    "        inversed_target[:, :n_cols] = final_target[:, :n_cols] * scale + mins\n",
    "\n",
    "        errors = torch.sqrt(\n",
    "            (inversed_preds[:, indices] - inversed_target[:, indices]) ** 2\n",
    "            + (inversed_preds[:, indices + 1] - inversed_target[:, indices + 1]) ** 2\n",
    "        )\n",
    "        self.sum += torch.sum(errors)\n",
    "        self.count += errors.size(0)\n",
    "\n",
    "    def compute(self):\n",
    "        return self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "\n",
    "class LitBiGRU(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, input_dim, hidden_dim, output_dim, num_layers, seq_length, lr=0.001\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim * seq_length)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.euclidean_distance = EuclideanDistance(cols, min_max_d)\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        last_out = out[:, -1, :]\n",
    "        output = self.fc(last_out)\n",
    "        return output.view(-1, 30, self.hparams.output_dim)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\n",
    "            \"train_euclidean_distance\",\n",
    "            self.euclidean_distance(y_hat, y),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\n",
    "            \"val_euclidean_distance\",\n",
    "            self.euclidean_distance(y_hat, y),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_path = \"<model_path>\"\n",
    "\n",
    "model = torch.load(model_path)\n",
    "# もしくは\n",
    "# model = LitBiGRU.load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提出データの予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualizer\n",
    "\n",
    "\n",
    "def f():\n",
    "    target = submission\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        preds = model(torch.tensor(target).to(device=device))\n",
    "        preds = preds.cpu().numpy()\n",
    "\n",
    "    tareget_df = []\n",
    "    for i in range(len(preds)):\n",
    "        df = pd.DataFrame(preds[i], columns=cols)\n",
    "        for j in df.columns:\n",
    "            df[j] = min_max_d[j].inverse(df[j])\n",
    "        tareget_df.append(df)\n",
    "\n",
    "    # concat\n",
    "    # display(tareget_df)\n",
    "\n",
    "    dfs = []\n",
    "    for index, file_name in enumerate(dataset_files):\n",
    "        pred_df = tareget_df[index]\n",
    "        sub_df = pd.read_csv(f\"/root/robocup/submission_data/{file_name}\")\n",
    "\n",
    "        sub_cols = sub_df.columns\n",
    "        df = pd.concat([sub_df, pred_df])\n",
    "        df = df[sub_cols]\n",
    "        dfs.append(df)\n",
    "\n",
    "    # index rest\n",
    "    for index, df in enumerate(dfs):\n",
    "        df.index = range(1, len(df) + 1)\n",
    "\n",
    "    # df[#] = index\n",
    "    for index, df in enumerate(dfs):\n",
    "        df[\"#\"] = df.index\n",
    "\n",
    "    for index, df in enumerate(dfs):\n",
    "        dfs[index] = df[\n",
    "            [\n",
    "                \"#\",\n",
    "                \"l1_x\",\n",
    "                \"l1_y\",\n",
    "                \"l2_x\",\n",
    "                \"l2_y\",\n",
    "                \"l3_x\",\n",
    "                \"l3_y\",\n",
    "                \"l4_x\",\n",
    "                \"l4_y\",\n",
    "                \"l5_x\",\n",
    "                \"l5_y\",\n",
    "                \"l6_x\",\n",
    "                \"l6_y\",\n",
    "                \"l7_x\",\n",
    "                \"l7_y\",\n",
    "                \"l8_x\",\n",
    "                \"l8_y\",\n",
    "                \"l9_x\",\n",
    "                \"l9_y\",\n",
    "                \"l10_x\",\n",
    "                \"l10_y\",\n",
    "                \"l11_x\",\n",
    "                \"l11_y\",\n",
    "                \"r1_x\",\n",
    "                \"r1_y\",\n",
    "                \"r2_x\",\n",
    "                \"r2_y\",\n",
    "                \"r3_x\",\n",
    "                \"r3_y\",\n",
    "                \"r4_x\",\n",
    "                \"r4_y\",\n",
    "                \"r5_x\",\n",
    "                \"r5_y\",\n",
    "                \"r6_x\",\n",
    "                \"r6_y\",\n",
    "                \"r7_x\",\n",
    "                \"r7_y\",\n",
    "                \"r8_x\",\n",
    "                \"r8_y\",\n",
    "                \"r9_x\",\n",
    "                \"r9_y\",\n",
    "                \"r10_x\",\n",
    "                \"r10_y\",\n",
    "                \"r11_x\",\n",
    "                \"r11_y\",\n",
    "                \"b_x\",\n",
    "                \"b_y\",\n",
    "            ]\n",
    "        ]\n",
    "    # last 30\n",
    "    for index, df in enumerate(dfs):\n",
    "        dfs[index] = df.iloc[-30:]\n",
    "    # save csv\n",
    "    os.makedirs(\"/root/robocup/submission_data_out\", exist_ok=True)\n",
    "    for index, df in enumerate(dfs):\n",
    "        df.to_csv(\n",
    "            f\"/root/robocup/submission_data_out/{dataset_files[index]}\", index=False\n",
    "        )\n",
    "\n",
    "    return dfs\n",
    "\n",
    "\n",
    "dfs = f()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
